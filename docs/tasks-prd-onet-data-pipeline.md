## Relevant Files

- `src/config/schemas.py` - Defines SQLAlchemy models for database tables (`Occupations`, `Skills`, `Scales`, `OnetApiOccupationData`, `OnetApiSkillsData`).
- `src/functions/mysql_init_tables.py` - Initializes database tables using SQLAlchemy models.
- `tests/test_integration_mysql_init_tables.py` - Integration tests for `mysql_init_tables.py`.
- `src/functions/extract_onet_data.py` - Extracts data from O*NET `.txt` files into pandas DataFrames.
- `tests/test_integration_extract_onet_data.py` - Integration tests for `extract_onet_data.py`.
- `src/functions/mysql_load_dataframe.py` - Loads a single pandas DataFrame into a specified MySQL table.
- `tests/test_integration_mysql_load_dataframe.py` - Integration tests for `mysql_load_dataframe.py`.
- `src/functions/textfile_to_dataframe.py` - **NEW:** Utility function to convert text files to DataFrames with proper typing.
- `src/functions/onet_api_extract_occupation.py` - **UPDATED:** Fetches O*NET-SOC occupation codes, titles, and descriptions from the API with optional filtering.
- `tests/test_integration_onet_api_extract_occupation.py` - **UPDATED:** Integration tests for `onet_api_extract_occupation.py` including filter tests.
- `src/functions/extract_onet_api_occupation_details.py` - **NEW:** Fetches detailed occupation data (description) for a list of occupation codes from the API.
- `tests/test_integration_extract_onet_api_occupation_details.py` - **NEW:** Integration tests for `extract_onet_api_occupation_details.py`.
- `src/functions/extract_onet_api_skills_data.py` - **NEW:** Parses skills data from detailed occupation API responses.
- `tests/test_integration_extract_onet_api_skills_data.py` - **NEW:** Integration tests for `extract_onet_api_skills_data.py`.
- `src/functions/get_onet_scales_reference.py` - **NEW:** Retrieves O*NET Scales Reference data (direct download or embedded).
- `tests/test_integration_get_onet_scales_reference.py` - **NEW:** Integration tests for `get_onet_scales_reference.py`.
- `src/functions/mysql_upsert_dataframe.py` - **NEW:** Upserts a pandas DataFrame into a specified MySQL table.
- `tests/test_integration_mysql_upsert_dataframe.py` - **NEW:** Integration tests for `mysql_upsert_dataframe.py`.
- `src/functions/populate_skills_reference.py` - **NEW:** Populates the Skills table from raw data tables.
- `src/functions/populate_occupation_skills.py` - **NEW:** Populates the Occupation_Skills table from raw data tables.
- `src/functions/llm_skill_profiler.py` - To house the LLM interaction logic for skill proficiency scoring.
- `tests/test_integration_llm_skill_profiler.py` - Integration tests for `llm_skill_profiler.py`.
- `src/nodes/extract_load.py` - Orchestrates text file data extraction and loading.
- `src/nodes/transform.py` - **NEW:** Orchestrates data transformation between raw and downstream tables.
- `src/scripts/transform.sh` - **NEW:** Shell script to run the transform node.
- `tests/test_integration_extract_load_text_files.py` - Integration tests for `extract_load_text_files.py`.
- `src/nodes/extract_load_api_data.py` - **NEW:** Orchestrates O*NET API data extraction and loading.
- `tests/test_integration_extract_load_api_data.py` - **NEW:** Integration tests for `extract_load_api_data.py`.
- `src/nodes/enrich_skill_data.py` - Orchestrates LLM enrichment of skill proficiency data.
- `tests/test_integration_enrich_skill_data.py` - Integration tests for `enrich_skill_data.py`.
- `src/functions/get_occupation_skills.py` - Retrieves occupation skills data for API.
- `tests/test_integration_get_occupation_skills.py` - Integration tests for `get_occupation_skills.py`.
- `src/functions/identify_skill_gap.py` - Contains the core logic for comparing two occupations' skills.
- `tests/test_integration_identify_skill_gap.py` - Integration tests for `identify_skill_gap.py`.
- `src/api/main.py` - Main FastAPI/Flask application file for the REST API.
- `src/api/routers/skill_gap.py` - API router/controller for the `/skill-gap` endpoint.
- `tests/test_api_skill_gap.py` - Integration tests for the `/skill-gap` API endpoint.
- `docker-compose.yml` - Defines and configures services (MySQL, ETL, API).
- `Dockerfile.api` - Dockerfile for the API service.
- `Dockerfile.etl` - Dockerfile for the ETL service.
- `requirements.txt` - Python project dependencies.
- `env/env.env` - Environment variable configuration.
- `README.md` - Project documentation.

### Notes

- Unit tests (if any) are typically alongside the code or in `tests/unit/`.
- Integration tests are in `tests/test_integration_<module_name>.py` and run via their `.sh` scripts.
- Use `python -m pytest` for running tests.

## Tasks

- [x] 1.0 **Phase 1: Setup & Text File ETL Foundational Components**
  - [x] 1.1 Define environment variables for DB connection (`env/env.env`).
  - [x] 1.2 Define SQLAlchemy schemas in `src/config/schemas.py` for `Occupations`, `Skills`, `Scales` tables (from text files).
  - [x] 1.3 Create function `mysql_init_tables(engine, tables_to_create: list)` in `src/functions/mysql_init_tables.py`. Inputs: SQLAlchemy engine, list of table model classes. Outputs: `{"success": bool, "message": str}`. Initializes specified tables.
  - [x] 1.4 Create integration test for `mysql_init_tables` (`tests/test_integration_mysql_init_tables.py` and `.sh` script).
  - [x] 1.5 Create function `extract_onet_data(file_mapping: dict)` in `src/functions/extract_onet_data.py`. Inputs: dict mapping file key (e.g., 'occupations') to file path. Outputs: `{"success": bool, "message": str, "result": {"occupations_df": pd.DataFrame, ...}}`. Reads O*NET `.txt` files into DataFrames.
  - [x] 1.6 Create integration test for `extract_onet_data` (`tests/test_integration_extract_onet_data.py` and `.sh` script).
  - [x] 1.7 Create function `mysql_load_dataframe(df: pd.DataFrame, table_name: str, engine, if_exists: str = 'replace')` in `src/functions/mysql_load_dataframe.py`. Inputs: DataFrame, table name, SQLAlchemy engine, if_exists strategy. Outputs: `{"success": bool, "message": str}`. Loads DataFrame to MySQL.
  - [x] 1.8 Create integration test for `mysql_load_dataframe` (`tests/test_integration_mysql_load_dataframe.py` and `.sh` script) using `Occupations` table as an example.
  - [x] 1.9 Create node `extract_load_text_files.py` in `src/nodes/`. This node will use `mysql_init_tables`, `extract_onet_data`, and `mysql_load_dataframe` to initialize tables and load all three text files (`Occupations`, `Skills`, `Scales`).
  - [x] 1.10 Create integration test for `extract_load_text_files` node (`tests/test_integration_extract_load_text_files.py` and `.sh` script).
  - [x] 1.11 Review and Refine Phase 1 functions and node for clarity, efficiency, docstrings, and adherence to rules.
  - [x] 1.12 Update `README.md` with setup/run instructions for Phase 1 text file ETL.
  - [x] 1.13 **NEW:** Create utility function `textfile_to_dataframe()` to simplify file processing.
  - [x] 1.14 **NEW:** Refactor `extract_onet_data.py` to use specialized extraction functions and the new utility.

- [ ] 2.0 **Phase 2: Data Ingestion - O*NET API Integration Functions (Supporting Bulk and On-Demand/Filtered Fetching)**
  - [x] 2.1 **MODIFIED (On-Demand):** Update function `onet_api_extract_occupation(username: str, password: str, filter_params: Optional[list] = None, ...)` in `src/functions/onet_api_extract_occupation.py`.
    - Inputs: API creds, optional list of filter strings (e.g., `["onetsoc_code.eq.CODE"]`).
    - Logic: If `filter_params` are provided, include them in the API request. Continues to support pagination for all results (filtered or unfiltered).
    - Outputs: `{"success": bool, "message": str, "result": {"occupation_df": pd.DataFrame}}`.
    - **IMPLEMENTATION NOTE:** Successfully updated to handle pagination and concatenate results from all pages.
  - [x] 2.1.1 **NEW (On-Demand):** Create integration test specifically for filtered `onet_api_extract_occupation` to verify it correctly fetches single or specific records based on filters (e.g., by `onetsoc_code`).
    - **IMPLEMENTATION NOTE:** Successfully tested with `["onetsoc_code.eq.15-1254.00"]` filter to fetch Web Developers occupation.
  - [x] 2.2 Create/Update integration test for bulk `onet_api_extract_occupation` (and its loading) (`tests/test_integration_api_extract_load_occupations.py` and `.sh`).
  - [ ] 2.3 **MODIFIED (On-Demand):** Review/Update `extract_onet_api_occupation_details` (if still used directly, or its logic incorporated elsewhere) to potentially leverage filtering if fetching for a *single* known occupation code. This might be superseded by direct filtered calls in the on-demand flow.
  - [x] 2.4 **NEW (On-Demand):** Design and implement similar `filter_params` and specific filter tests for `onet_api_extract_skills` and `onet_api_extract_scales` functions.
    - [x] 2.4.1 Update `onet_api_extract_skills` to accept `filter_params` and add integration test for filtered skill extraction.
      - **IMPLEMENTATION NOTE:** Updated function to accept filters and handle pagination similarly to the occupation extraction.
    - [ ] 2.4.2 Update `onet_api_extract_scales` to accept `filter_params` and add integration test for filtered scale extraction.
      - **IMPLEMENTATION NOTE:** Scales are static reference data that doesn't need occupation-specific filtering.
  - [x] 2.5 Create function `onet_api_extract_skills_data(occupation_details_df: pd.DataFrame)` in `src/functions/onet_api_extract_skills_data.py`. (This function parses XML details; may need adjustment if detailed occupation data is fetched differently in on-demand flow).
  - [x] 2.7 Create function `get_onet_scales_reference(url: str)` in `src/functions/get_onet_scales_reference.py`.
  - [x] 2.11 Define SQLAlchemy schemas for the API data landing tables in `src/config/schemas.py`.
  - [x] 2.13 Create node `extract_load_api.py` in `src/nodes/`. This node is for *bulk* API data extraction using the updated functions (without filters or with broad filters if ever needed for bulk).
  - [x] 2.14 Create integration test for `extract_load_api.py` node.
  - [x] 2.15 **NEW (On-Demand):** Document the on-demand pull strategy with caching in `memory_bank/notes.md`.
    - **IMPLEMENTATION NOTE:** Added detailed notes on the strategy shift to on-demand pulling with local caching.

- [x] 3.0 **Phase 3: Database Normalization & Downstream Consumption Tables**
  - [x] 3.1 Analyze data from both sources (text files and API) to understand their structure and relationships.
  - [x] 3.2 Design and implement simplified downstream tables from existing raw data tables.
    - [x] 3.2.1 Reuse existing `Occupations` table instead of creating a new normalized table.
    - [x] 3.2.2 Create `Skills` table to store unique skills information.
    - [x] 3.2.3 Create `Occupation_Skills` table as a joining table between occupations and skills.
  - [x] 3.3 Create data transformation functions to populate downstream tables from raw data tables.
    - [x] 3.3.1 Create `populate_skills_reference()` function to populate Skills table from raw data.
    - [x] 3.3.2 Create `populate_occupation_skills()` function to populate Occupation_Skills table from raw data.
    - [x] 3.3.3 Create `transform.py` node to chain these functions together.
    - [x] 3.3.4 Create shell script `transform.sh` to run the transform node.
  - [x] 3.4 Update `schemas.py` to ensure all table definitions match the actual database structure.
  - [x] 3.5 Create `get_occupation` function to retrieve occupation data from downstream tables.
  - [x] 3.6 Refactor `get_occupation_skills` function to pull data from downstream tables.
  - [x] 3.7 Create integration tests for downstream tables and updated functions.
    - [x] 3.7.1 Create integration test for `populate_skills_reference()`.
    - [x] 3.7.2 Create integration test for `populate_occupation_skills()`.
    - [x] 3.7.3 Update integration test for `get_occupation()` and `get_occupation_skills()`.
  - [x] 3.8 Document simplified data model showing flow from raw to downstream tables.

- [ ] 4.0 **Phase 4: LLM Integration for Skill Proficiency**
  - [ ] 4.1 Research and select/configure LLM for skill proficiency analysis.
  - [ ] 4.2 Create function `llm_skill_profiler(skill_data: pd.DataFrame, text_column: str)` in `src/functions/llm_skill_profiler.py`. Inputs: DataFrame with skill info (e.g., from `SkillsView`), column name with text for LLM. Outputs: `{"success": bool, "message": str, "result": {"llm_scores_df": pd.DataFrame}}` with original data + LLM scores.
  - [ ] 4.3 Create integration test for `llm_skill_profiler` (`tests/test_integration_llm_skill_profiler.py` and `.sh`). (May require mocking LLM calls for CI/CD).
  - [ ] 4.4 Design schema for LLM-enriched data storage, if needed, based on actual LLM output structure.
  - [ ] 4.5 Create node `enrich_skill_data.py` in `src/nodes/`. This node will:
    - [ ] 4.5.1 Read data (e.g., from `SkillsView`, filtering for `scale_id` = 'LV').
    - [ ] 4.5.2 Call `llm_skill_profiler`.
    - [ ] 4.5.3 Upsert LLM-derived `data_value` back into a relevant table or a new `LLMSkillScores` table using `mysql_upsert_dataframe`.
  - [ ] 4.6 Create integration test for `enrich_skill_data` node (`tests/test_integration_enrich_skill_data.py` and `.sh`).

- [ ] 5.0 **Phase 5: REST API for Skill Gap Analysis (Incorporating On-Demand API Fetching & Caching)**
  - [ ] 5.1 Set up FastAPI framework in `src/api/main.py`.
  - [ ] 5.2 **NEW (On-Demand):** Define strategy for caching API-fetched data. This includes deciding where (e.g., existing landing tables or new dedicated cache tables) and how (e.g., simple insert on miss, TTL).
  - [ ] 5.3 **NEW (On-Demand):** Create/Refactor a function, e.g., `ensure_occupation_data_is_present(onet_soc_code: str, engine, api_credentials: dict)` that:
    - Checks local DB (downstream tables like `Occupations`, `Occupation_Skills`) for the given `onet_soc_code`.
    - If data is missing or incomplete for the analysis:
        - Calls the modified API extraction functions (`onet_api_extract_occupation`, `onet_api_extract_skills`, etc.) using `filter_params` to fetch data for the specific `onet_soc_code`.
        - Loads the fetched data into the API landing tables (e.g., `Onet_Occupations_API_landing`).
        - Triggers a transformation/population process (similar to `transform.py` node, but targeted for the new data) to move data from landing tables to downstream consumption tables (`Occupations`, `Skills`, `Occupation_Skills`).
        - Returns a success/failure status for data presence.
  - [ ] 5.4 **MODIFIED (On-Demand):** Update function `get_occupation_skills(onet_soc_code: str, scale_id_filter: str, engine, api_credentials: dict)` in `src/functions/get_occupation_skills.py`.
    - Inputs: occupation code, scale ID to filter (e.g., 'LV'), SQLAlchemy engine, API credentials.
    - Logic:
        - Call `ensure_occupation_data_is_present()` for the `onet_soc_code`.
        - If successful, retrieve occupation skills data from local downstream tables.
    - Outputs: `{"success": bool, "message": str, "result": {"skills_df": pd.DataFrame}}`. Uses `SkillsView` or LLM-enriched data.
  - [ ] 5.5 Create integration test for the updated `get_occupation_skills`. This test should cover scenarios where data is purely local, and where on-demand API fetching and caching is triggered.
  - [ ] 5.6 Review `identify_skill_gap.py`. Ensure it takes two DataFrames (from `get_occupation_skills`) as input.
  - [ ] 5.7 Create/Update integration test for `identify_skill_gap`.
  - [ ] 5.8 Implement `GET /skill-gap` endpoint in `src/api/routers/skill_gap.py` using the functions above. Ensure it passes API credentials (if needed by `get_occupation_skills`) securely.
  - [ ] 5.9 Create integration test for `/skill-gap` API endpoint.

- [ ] 6.0 **Phase 6: Containerization, Final Testing, and Documentation**
  - [ ] 6.1 Update `docker-compose.yml` for all services (DB, API, ETL nodes as services/jobs).
  - [ ] 6.2 Create/Update `Dockerfile.api`, `Dockerfile.etl`.
  - [ ] 6.3 Finalize `requirements.txt`.
  - [ ] 6.4 Ensure all integration tests pass in Docker environment.
  - [ ] 6.5 Update `README.md` (full setup, schema, data flow, API examples, design decisions).
  - [ ] 6.6 Implement comprehensive error handling and logging across all components. 